{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pleasant-witch",
   "metadata": {},
   "source": [
    "# Q-Learning in Reinforcement Learning\n",
    "\n",
    "Q-Learning a simple but important algorithms to learn reinforcement learning.\n",
    "\n",
    "In order to understand how it works, first let's consider the expected rewards as follows.\n",
    "\n",
    "$$ R = \\sum_{t=0}^{\\infty} {\\gamma^t r_t} $$\n",
    "\n",
    "where $r_t$ is a reward value obtained at $t$ and $\\gamma$ is discount.\n",
    "\n",
    "For instance, when you try to grab an object, you will do the following 3 actions :\n",
    "\n",
    "- action #1 : Stretch your arm ($t=0$)<br>\n",
    "  Getting reward 0.\n",
    "- action #2 : Open your hand ($t=1$)<br>\n",
    "  Getting reward 0.\n",
    "- action #3 : Grab an object ($t=2$)<br>\n",
    "  Getting reward 10.\n",
    "\n",
    "In this case, you will get a reward value 10 on action #3 ($t=2$), however the action #1 ($t=0$) is obviously contributing to the final rewards. Hence, we consider that the action #1 will have the following expected cumulative reward.<br>\n",
    "Here we assume $\\gamma$ is 0.99.\n",
    "\n",
    "$$ R_{t=0} = 0 + 0.99 \\times 0 + 0.99^2 \\times 10 = 9.801 $$\n",
    "\n",
    "Same as above, $R_{t=1} = 9.9, R_{t=2} = 10$.\n",
    "\n",
    "Q-value is based on this idea of expected cumulative reward. Depending on each state (observation), each action will have the corresponding expected reward.<br>\n",
    "In above example, if you see an object in front of you (i.e, the **state** of \"you see an object\"), the **action** \"stretching your arm\" will have high value of expected reward. However, if you cannot see an object anywhere, the action \"stretching your arm\" will have low value of expected reward.\n",
    "\n",
    "Q-value of each corresponding state and action is denoted as $Q(s, a)$. Suppose both action and state has 1 dimension of discrete values, $Q(s, a)$ will be written as a table (called Q-Table) as follows.<br>\n",
    "If the state is s2, the optimal action to pick up will be action a2. If s3, the optimal action will be action a4.\n",
    "\n",
    "![Q-Table](assets/q-table.png)\n",
    "\n",
    "In practice, both action space and observation space may have more than 1 dimension. Then Q-Table will be the combination of 1 dimension (action space) and 4 dimension (observation space).\n",
    "\n",
    "In Q-Learning, we optimize this table by the following iterative updates ($t=0,1,2,\\ldots$).<br>\n",
    "In the following equation, $ Q_t(s_t,a_t) $ is current Q-value and $ Q_{t+1}(s_t,a_t) $ is the updated Q-value.\n",
    "\n",
    "$$ Q_{t+1}(s_t,a_t) = Q_t(s_t,a_t) + \\alpha \\left( r_t + \\gamma \\max_a{Q_t(s_{t+1},a)} - Q_t(s_t,a_t) \\right) $$\n",
    "\n",
    "where $\\alpha$ is learning rate.\n",
    "\n",
    "This equation means that :\n",
    "\n",
    "- Suppose, you executed an action $a_t$ on state $s_t$, and as a result, you got reward $r_t$ and the state has changed to $s_{t+1}$.\n",
    "- The optimal next action will satisfy $a_{t+1}=\\max_{a}{Q(s_{t+1},a)}$.<br>\n",
    "  By taking this optimal action, you will then get the expected reward : $r_t + \\gamma \\max_{a}{Q(s_{t+1},a)}$.\n",
    "- Compare this optimal q-value with current q-value $Q(s_t,a_t)$ in q-table. Then update this current value $Q(s_t,a_t)$ by learning rate $\\alpha$.<br>\n",
    "  This will result into above equation.\n",
    "\n",
    "We will try q-learning with the frozen lake Environment from the gym library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "creative-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import typing as tt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-perspective",
   "metadata": {},
   "source": [
    "## Frozen lake environment\n",
    "Frozen lake involves crossing a frozen lake from Start(S) to Goal(G) without falling into any Holes(H) by walking over the Frozen(F) lake. The agent may not always move in the intended direction due to the slippery nature of the frozen lake.\n",
    "\n",
    "![FrozenLake](assets/frozen_lake.gif)\n",
    "\n",
    "### Action Space\n",
    "The agent takes a 1-element vector for actions. The action space is (dir), where dir decides direction to move in which can be:\n",
    "\n",
    "### Observation Space\n",
    "The observation is a value representing the agentâ€™s current position as current_row * nrows + current_col (where both the row and col start at 0). For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15. The number of possible observations is dependent on the size of the map. For example, the 4x4 map has 16 possible observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884357ec",
   "metadata": {},
   "source": [
    "# Let's play arround with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "honey-house",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation= 1 Reward= 0.0 Done= False\n",
      "Observation= 1 Reward= 0.0 Done= False\n",
      "Observation= 0 Reward= 0.0 Done= False\n",
      "Observation= 4 Reward= 0.0 Done= False\n",
      "Observation= 8 Reward= 0.0 Done= False\n",
      "Observation= 8 Reward= 0.0 Done= False\n",
      "Observation= 12 Reward= 0.0 Done= True\n",
      "TERMINIERT!\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode='human')\n",
    "observation = env.reset() \n",
    "# env.action_space => Discrete(4) => 0=left, 1=down, 2=right, 3=up\n",
    "# env.observation_space => Discrete(16) => numbered 4x4 box\n",
    "\n",
    "pathlist = []\n",
    "for _ in range(100): # 100 number of steps\n",
    "  env.render() # show position of the robot\n",
    "  \n",
    "  action = randint(1,2) # Down/right Strategy\n",
    "  pathlist.append(action) \n",
    "  observation, reward, done, info, _ = env.step(action) \n",
    "  # observation = 0-15 pos, reward = 0.0 or 1.0, done = true / false, info=prob=0.333\n",
    "  print(\"Observation=\", observation, \"Reward=\", reward, \"Done=\", done)\n",
    "  if reward > 0.0:\n",
    "    print(\"GEWINNPFAD: \", pathlist, \", LEN = \" , len(pathlist))\n",
    "    break;\n",
    "\n",
    "  if done:\n",
    "    print(\"TERMINIERT!\")\n",
    "    break\n",
    "\n",
    "env.render()\n",
    "observation = env.reset() # shut down\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e2f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close() # shut down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605d4e88",
   "metadata": {},
   "source": [
    "# Now, Lets take a look at Q-learning with tabular learning\n",
    "\n",
    "##Exercise 1\n",
    "\n",
    "1. Creaate the Q-Table\n",
    "2. Implement a Q-learning algirithm for the Frozen Lake environment. Evaluate your results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17260775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creaate the Q-Table\n",
    "q_table = # Enter your code here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "needed-communications",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Implement a Q-learning algirithm for the Frozen Lake environment. Evaluate your results  \n",
    "\n",
    "\n",
    "gamma = 0.9\n",
    "alpha = 0.5\n",
    "\n",
    "\n",
    "# pick up action from q-table\n",
    "def pick_sample(s):\n",
    "    if np.max(q_table[s]) > 0:\n",
    "        a = np.argmax(q_table[s])\n",
    "    else:\n",
    "        a = env.action_space.sample()\n",
    "    return a\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\") # is_slippery=False, render_mode='human'\n",
    "q_table = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "#Q-Table befor Training\n",
    "print(q_table)\n",
    "outcomes = []\n",
    "reward_records = []\n",
    "for i in range(1000):\n",
    "    # Run episode till done\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    s, _ = env.reset()\n",
    "    \n",
    "           \n",
    "    while not done:\n",
    "        a = pick_sample(s)\n",
    "        s_new, r, term, trunc, _ = env.step(a)\n",
    "        done = term or trunc\n",
    "        \n",
    "        # Update Q-Table\n",
    "        \n",
    "        #----Enter your code here for updating the q table---------#\n",
    "        #Hint: Use Equation frome#\n",
    "        \n",
    "        total_reward += r\n",
    "        s = s_new\n",
    "\n",
    "   \n",
    "    # Record total rewards in episode (max 500)\n",
    "    print(\"Run episode{} with rewards {}\".format(i, total_reward), end=\"\\r\")\n",
    "    reward_records.append(total_reward)\n",
    "\n",
    "print(\"\\nDone\")\n",
    "env.close()\n",
    "#Q-Table after training\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4ad335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Generate recent 50 interval average\n",
    "average_reward = []\n",
    "for idx in range(len(reward_records)):\n",
    "    avg_list = np.empty(shape=(1,), dtype=int)\n",
    "    if idx < 50:\n",
    "        avg_list = reward_records[:idx+1]\n",
    "    else:\n",
    "        avg_list = reward_records[idx-49:idx+1]\n",
    "    average_reward.append(np.average(avg_list))\n",
    "# Plot\n",
    "plt.plot(reward_records)\n",
    "plt.plot(average_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fitted-torture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run episode5999 with rewards 500.0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the learned Policy wwith 100 Episodes\n",
    "\n",
    "episodes = 100\n",
    "nb_success = 0\n",
    "\n",
    "# Evaluation\n",
    "for _ in range(100):\n",
    "    s, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # Until the agent gets stuck or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        # Choose the action with the highest value in the current state\n",
    "        if np.max(q_table[s]) > 0:\n",
    "          a = np.argmax(q_table[s])\n",
    "\n",
    "        # If there's no best action (only zeros), take a random one\n",
    "        else:\n",
    "          a = env.action_space.sample()\n",
    "             \n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        s_new, r, term, trunc, _  = env.step(a)\n",
    "        done = term or trunc\n",
    "        # Update our current state\n",
    "        s = s_new\n",
    "\n",
    "        # When we get a reward, it means we solved the game\n",
    "        nb_success += r\n",
    "\n",
    "# Evaluation\n",
    "# Let's check our success rate!\n",
    "print (f\"Success rate = {nb_success/episodes*100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-champagne",
   "metadata": {},
   "source": [
    "# Epsilon-Greedy algorithm\n",
    "\n",
    "With the previos approach the agent always chooses the action with the highest value. So, whenever a state-action pair starts having a non-zero value, the agent will always choose it. The other actions will never be taken, which means weâ€™ll never update their valueâ€¦ But what if one of these actions was better than the one the agent always takes? Shouldnâ€™t we encourage the agent to try new things from time to time and see if it can improve?\n",
    "In other words, we want to allow our agent to either:\n",
    "\n",
    "- Take the action with the highest value (exploitation);\n",
    "- Choose a random action to try to find even better ones (exploration).\n",
    "![Epsilon Greedy](assets/epsilon_greedy.gif)\n",
    "\n",
    "\n",
    "## Excercise 2\n",
    "\n",
    "\n",
    "1. Adjust the pick_sample function such that we have an epsilon-greedy algorithm\n",
    "2. Evaluate the training results and compare to the plain Q-learning approach \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 4000       # Total number of episodes\n",
    "alpha = 0.5            # Learning rate\n",
    "gamma = 0.9            # Discount factor\n",
    "epsilon = 1.0          # Amount of randomness in the action selection\n",
    "epsilon_decay = 0.001  # Fixed amount to decrease\n",
    "\n",
    "\n",
    "def pick_sample(s, epsilon):\n",
    "    # get optimal action,\n",
    "    # but with greedy exploration (to prevent picking up same values in the first stage)\n",
    "    \n",
    "    # ----- Insert your Code here ------ #\n",
    "    \n",
    "    return a\n",
    "\n",
    "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "reward_records = []\n",
    "for _ in range(episodes):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    s, _ = env.reset()\n",
    "    \n",
    "   \n",
    "    # Until the agent gets stuck in a hole or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        a = pick_sample(s, epsilon)\n",
    "        s_new, r, term, trunc, _ = env.step(a)\n",
    "        done = term or trunc\n",
    "\n",
    "       \n",
    "        # Update Q-Table\n",
    "        # Update Q-Table\n",
    "        maxQ = np.max(q_table[s_new])\n",
    "        q_table[(s)][a] = q_table[(s)][a] + alpha * (r + gamma * maxQ - q_table[s][a])\n",
    "        \n",
    "        total_reward += r\n",
    "        s = s_new\n",
    "\n",
    "    print(\"Run episode{} with rewards {}\".format(i, total_reward), end=\"\\r\")\n",
    "    reward_records.append(total_reward)\n",
    "\n",
    "    # Update epsilon\n",
    "    epsilon = max(epsilon - epsilon_decay, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_reward = []\n",
    "for idx in range(len(reward_records)):\n",
    "    avg_list = np.empty(shape=(1,), dtype=int)\n",
    "    if idx < 50:\n",
    "        avg_list = reward_records[:idx+1]\n",
    "    else:\n",
    "        avg_list = reward_records[idx-49:idx+1]\n",
    "    average_reward.append(np.average(avg_list))\n",
    "# Plot\n",
    "plt.plot(reward_records)\n",
    "plt.plot(average_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069b2d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "nb_success = 0\n",
    "\n",
    "# Evaluation\n",
    "for _ in range(100):\n",
    "    s, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # Until the agent gets stuck or reaches the goal, keep training it\n",
    "    while not done:\n",
    "        # Choose the action with the highest value in the current state\n",
    "        if np.max(q_table[s]) > 0:\n",
    "          a = np.argmax(q_table[s])\n",
    "\n",
    "        # If there's no best action (only zeros), take a random one\n",
    "        else:\n",
    "          a = env.action_space.sample()\n",
    "             \n",
    "        # Implement this action and move the agent in the desired direction\n",
    "        s_new, r, term, trunc, _  = env.step(a)\n",
    "        done = term or trunc\n",
    "        # Update our current state\n",
    "        s = s_new\n",
    "\n",
    "        # When we get a reward, it means we solved the game\n",
    "        nb_success += r\n",
    "\n",
    "# Evaluation\n",
    "# Let's check our success rate!\n",
    "print (f\"Success rate = {nb_success/episodes*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
